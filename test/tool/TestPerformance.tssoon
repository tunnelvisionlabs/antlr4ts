/*
 * [The "BSD license"]
 *  Copyright (c) 2012 Terence Parr
 *  Copyright (c) 2012 Sam Harwell
 *  All rights reserved.
 *
 *  Redistribution and use in source and binary forms, with or without
 *  modification, are permitted provided that the following conditions
 *  are met:
 *
 *  1. Redistributions of source code must retain the above copyright
 *     notice, this list of conditions and the following disclaimer.
 *  2. Redistributions in binary form must reproduce the above copyright
 *     notice, this list of conditions and the following disclaimer in the
 *     documentation and/or other materials provided with the distribution.
 *  3. The name of the author may not be used to endorse or promote products
 *     derived from this software without specific prior written permission.
 *
 *  THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 *  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 *  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 *  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 *  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 *  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

// ConvertTo-TS run at 2016-10-04T11:27:15.5869363-07:00

// import org.junit.Assert;
// import org.junit.Test;

// import static org.hamcrest.CoreMatchers.instanceOf;
// import static org.junit.Assert.assertThat;
// import static org.junit.Assert.assertTrue;

export class TestPerformance extends BaseTest {
    /**
     * Parse all java files under this package within the JDK_SOURCE_ROOT
     * (environment variable or property defined on the Java command line).
     */
    private static TOP_PACKAGE: string; 
	static {
		let packageForTesting: string =  System.getProperty("performance.package");
		if (packageForTesting == null) {
			packageForTesting = "java.lang";
		}

		TOP_PACKAGE = packageForTesting;
	}

    /**
     * {@code true} to load java files from sub-packages of
     * {@link #TOP_PACKAGE}.
     */
    private static RECURSIVE: boolean =  true;
	/**
	 * {@code true} to read all source files from disk into memory before
	 * starting the parse. The default value is {@code true} to help prevent
	 * drive speed from affecting the performance results. This value may be set
	 * to {@code false} to support parsing large input sets which would not
	 * otherwise fit into memory.
	 */
	private static PRELOAD_SOURCES: boolean =  true;
	/**
	 * The encoding to use when reading source files.
	 */
	private static ENCODING: string =  "UTF-8";
	/**
	 * The maximum number of files to parse in a single iteration.
	 */
	private static MAX_FILES_PER_PARSE_ITERATION: number =  Integer.MAX_VALUE;

	/**
	 * {@code true} to call {@link Collections#shuffle} on the list of input
	 * files before the first parse iteration.
	 */
	private static SHUFFLE_FILES_AT_START: boolean =  false;
	/**
	 * {@code true} to call {@link Collections#shuffle} before each parse
	 * iteration <em>after</em> the first.
	 */
	private static SHUFFLE_FILES_AFTER_ITERATIONS: boolean =  false;
	/**
	 * The instance of {@link Random} passed when calling
	 * {@link Collections#shuffle}.
	 */
	private static RANDOM: Random =  new Random();

    /**
     * {@code true} to use the Java grammar with expressions in the v4
     * left-recursive syntax (JavaLR.g4). {@code false} to use the standard
     * grammar (Java.g4). In either case, the grammar is renamed in the
     * temporary directory to Java.g4 before compiling.
     */
    private static USE_LR_GRAMMAR: boolean =  true;
    /**
     * {@code true} to specify the {@code -Xforce-atn} option when generating
     * the grammar, forcing all decisions in {@code JavaParser} to be handled by
     * {@link ParserATNSimulator#adaptivePredict}.
     */
    private static FORCE_ATN: boolean =  false;
    /**
     * {@code true} to specify the {@code -atn} option when generating the
     * grammar. This will cause ANTLR to export the ATN for each decision as a
     * DOT (GraphViz) file.
     */
    private static EXPORT_ATN_GRAPHS: boolean =  true;
	/**
	 * {@code true} to specify the {@code -XdbgST} option when generating the
	 * grammar.
	 */
	private static DEBUG_TEMPLATES: boolean =  false;
	/**
	 * {@code true} to specify the {@code -XdbgSTWait} option when generating the
	 * grammar.
	 */
	private static DEBUG_TEMPLATES_WAIT: boolean =  DEBUG_TEMPLATES;
    /**
     * {@code true} to delete temporary (generated and compiled) files when the
     * test completes.
     */
    private static DELETE_TEMP_FILES: boolean =  true;
	/**
	 * {@code true} to use a {@link ParserInterpreter} for parsing instead of
	 * generated parser.
	 */
	private static USE_PARSER_INTERPRETER: boolean =  false;

	/**
	 * {@code true} to call {@link System#gc} and then wait for 5 seconds at the
	 * end of the test to make it easier for a profiler to grab a heap dump at
	 * the end of the test run.
	 */
    private static PAUSE_FOR_HEAP_DUMP: boolean =  false;

    /**
     * Parse each file with {@code JavaParser.compilationUnit}.
     */
    private static RUN_PARSER: boolean =  true;
    /**
     * {@code true} to use {@link BailErrorStrategy}, {@code false} to use
     * {@link DefaultErrorStrategy}.
     */
    private static BAIL_ON_ERROR: boolean =  false;
	/**
	 * {@code true} to compute a checksum for verifying consistency across
	 * optimizations and multiple passes.
	 */
	private static COMPUTE_CHECKSUM: boolean =  true;
    /**
     * This value is passed to {@link Parser#setBuildParseTree}.
     */
    private static BUILD_PARSE_TREES: boolean =  false;
    /**
     * Use
     * {@link ParseTreeWalker#DEFAULT}{@code .}{@link ParseTreeWalker#walk walk}
     * with the {@code JavaParserBaseListener} to show parse tree walking
     * overhead. If {@link #BUILD_PARSE_TREES} is {@code false}, the listener
     * will instead be called during the parsing process via
     * {@link Parser#addParseListener}.
     */
    private static BLANK_LISTENER: boolean =  false;

    private static EXPORT_LARGEST_CONFIG_CONTEXTS: boolean =  false;

	/**
	 * Shows the number of {@link DFAState} and {@link ATNConfig} instances in
	 * the DFA cache at the end of each pass. If {@link #REUSE_LEXER_DFA} and/or
	 * {@link #REUSE_PARSER_DFA} are false, the corresponding instance numbers
	 * will only apply to one file (the last file if {@link #NUMBER_OF_THREADS}
	 * is 0, otherwise the last file which was parsed on the first thread).
	 */
    private static SHOW_DFA_STATE_STATS: boolean =  true;
	/**
	 * If {@code true}, the DFA state statistics report includes a breakdown of
	 * the number of DFA states contained in each decision (with rule names).
	 */
	private static DETAILED_DFA_STATE_STATS: boolean =  true;

	private static ENABLE_LEXER_DFA: boolean =  true;

	private static ENABLE_PARSER_DFA: boolean =  true;
	/**
	 * If {@code true}, the DFA will be used for full context parsing as well as
	 * SLL parsing.
	 */
	private static ENABLE_PARSER_FULL_CONTEXT_DFA: boolean =  false;

	/**
	 * Specify the {@link PredictionMode} used by the
	 * {@link ParserATNSimulator}. If {@link #TWO_STAGE_PARSING} is
	 * {@code true}, this value only applies to the second stage, as the first
	 * stage will always use {@link PredictionMode#SLL}.
	 */
    private static PREDICTION_MODE: PredictionMode =  PredictionMode.LL;
    private static FORCE_GLOBAL_CONTEXT: boolean =  false;
    private static TRY_LOCAL_CONTEXT_FIRST: boolean =  true;
	private static OPTIMIZE_LL1: boolean =  true;
	private static OPTIMIZE_UNIQUE_CLOSURE: boolean =  true;
	private static OPTIMIZE_TAIL_CALLS: boolean =  true;
	private static TAIL_CALL_PRESERVES_SLL: boolean =  true;
	private static TREAT_SLLK1_CONFLICT_AS_AMBIGUITY: boolean =  false;

	private static TWO_STAGE_PARSING: boolean =  true;

    private static SHOW_CONFIG_STATS: boolean =  false;

	/**
	 * If {@code true}, detailed statistics for the number of DFA edges were
	 * taken while parsing each file, as well as the number of DFA edges which
	 * required on-the-fly computation.
	 */
	private static COMPUTE_TRANSITION_STATS: boolean =  false;
	private static SHOW_TRANSITION_STATS_PER_FILE: boolean =  false;
	/**
	 * If {@code true}, the transition statistics will be adjusted to a running
	 * total before reporting the final results.
	 */
	private static TRANSITION_RUNNING_AVERAGE: boolean =  false;
	/**
	 * If {@code true}, transition statistics will be weighted according to the
	 * total number of transitions taken during the parsing of each file.
	 */
	private static TRANSITION_WEIGHTED_AVERAGE: boolean =  false;

	/**
	 * If {@code true}, after each pass a summary of the time required to parse
	 * each file will be printed.
	 */
	private static COMPUTE_TIMING_STATS: boolean =  false;
	/**
	 * If {@code true}, the timing statistics for {@link #COMPUTE_TIMING_STATS}
	 * will be cumulative (i.e. the time reported for the <em>n</em>th file will
	 * be the total time required to parse the first <em>n</em> files).
	 */
	private static TIMING_CUMULATIVE: boolean =  false;
	/**
	 * If {@code true}, the timing statistics will include the parser only. This
	 * flag allows for targeted measurements, and helps eliminate variance when
	 * {@link #PRELOAD_SOURCES} is {@code false}.
	 * <p/>
	 * This flag has no impact when {@link #RUN_PARSER} is {@code false}.
	 */
	private static TIME_PARSE_ONLY: boolean =  false;

	/**
	 * When {@code true}, messages will be printed to {@link System#err} when
	 * the first stage (SLL) parsing resulted in a syntax error. This option is
	 * ignored when {@link #TWO_STAGE_PARSING} is {@code false}.
	 */
	private static REPORT_SECOND_STAGE_RETRY: boolean =  true;
	private static REPORT_SYNTAX_ERRORS: boolean =  true;
	private static REPORT_AMBIGUITIES: boolean =  false;
	private static REPORT_FULL_CONTEXT: boolean =  false;
	private static REPORT_CONTEXT_SENSITIVITY: boolean =  REPORT_FULL_CONTEXT;

    /**
     * If {@code true}, a single {@code JavaLexer} will be used, and
     * {@link Lexer#setInputStream} will be called to initialize it for each
     * source file. Otherwise, a new instance will be created for each file.
     */
    private static REUSE_LEXER: boolean =  false;
	/**
	 * If {@code true}, a single DFA will be used for lexing which is shared
	 * across all threads and files. Otherwise, each file will be lexed with its
	 * own DFA which is accomplished by creating one ATN instance per thread and
	 * clearing its DFA cache before lexing each file.
	 */
	private static REUSE_LEXER_DFA: boolean =  true;
    /**
     * If {@code true}, a single {@code JavaParser} will be used, and
     * {@link Parser#setInputStream} will be called to initialize it for each
     * source file. Otherwise, a new instance will be created for each file.
     */
    private static REUSE_PARSER: boolean =  false;
	/**
	 * If {@code true}, a single DFA will be used for parsing which is shared
	 * across all threads and files. Otherwise, each file will be parsed with
	 * its own DFA which is accomplished by creating one ATN instance per thread
	 * and clearing its DFA cache before parsing each file.
	 */
	private static REUSE_PARSER_DFA: boolean =  true;
    /**
     * If {@code true}, the shared lexer and parser are reset after each pass.
     * If {@code false}, all passes after the first will be fully "warmed up",
     * which makes them faster and can compare them to the first warm-up pass,
     * but it will not distinguish bytecode load/JIT time from warm-up time
     * during the first pass.
     */
    private static CLEAR_DFA: boolean =  false;
    /**
     * Total number of passes to make over the source.
     */
    private static PASSES: number =  4;

	/**
	 * This option controls the granularity of multi-threaded parse operations.
	 * If {@code true}, the parsing operation will be parallelized across files;
	 * otherwise the parsing will be parallelized across multiple iterations.
	 */
	private static FILE_GRANULARITY: boolean =  true;

	/**
	 * Number of parser threads to use.
	 */
	private static NUMBER_OF_THREADS: number =  1;

    private static sharedLexers: Lexer[] =  new Lexer[NUMBER_OF_THREADS];
	private static sharedLexerATNs: ATN[] =  new ATN[NUMBER_OF_THREADS];

    private static sharedParsers: Parser[] =  new Parser[NUMBER_OF_THREADS];
	private static sharedParserATNs: ATN[] =  new ATN[NUMBER_OF_THREADS];

    private static sharedListeners: ParseTreeListener[] =  new ParseTreeListener[NUMBER_OF_THREADS];

	private static totalTransitionsPerFile: number[][]; 
	private static computedTransitionsPerFile: number[][]; 
	static {
		if (COMPUTE_TRANSITION_STATS) {
			totalTransitionsPerFile = new long[PASSES][];
			computedTransitionsPerFile = new long[PASSES][];
		} else {
			totalTransitionsPerFile = null;
			computedTransitionsPerFile = null;
		}
	}

	private static decisionInvocationsPerFile: number[][][]; 
	private static fullContextFallbackPerFile: number[][][]; 
	private static nonSllPerFile: number[][][]; 
	private static totalTransitionsPerDecisionPerFile: number[][][]; 
	private static computedTransitionsPerDecisionPerFile: number[][][]; 
	private static fullContextTransitionsPerDecisionPerFile: number[][][]; 
	static {
		if (COMPUTE_TRANSITION_STATS && DETAILED_DFA_STATE_STATS) {
			decisionInvocationsPerFile = new long[PASSES][][];
			fullContextFallbackPerFile = new long[PASSES][][];
			nonSllPerFile = new long[PASSES][][];
			totalTransitionsPerDecisionPerFile = new long[PASSES][][];
			computedTransitionsPerDecisionPerFile = new long[PASSES][][];
			fullContextTransitionsPerDecisionPerFile = new long[PASSES][][];
		} else {
			decisionInvocationsPerFile = null;
			fullContextFallbackPerFile = null;
			nonSllPerFile = null;
			totalTransitionsPerDecisionPerFile = null;
			computedTransitionsPerDecisionPerFile = null;
			fullContextTransitionsPerDecisionPerFile = null;
		}
	}

	private static timePerFile: number[][]; 
	private static tokensPerFile: number[][]; 
	static {
		if (COMPUTE_TIMING_STATS) {
			timePerFile = new long[PASSES][];
			tokensPerFile = new int[PASSES][];
		} else {
			timePerFile = null;
			tokensPerFile = null;
		}
	}

    private tokenCount: AtomicIntegerArray =  new AtomicIntegerArray(PASSES);

    @Test
    //@org.junit.Ignore
    compileJdk(): void, InterruptedException, ExecutionException {
        let jdkSourceRoot: string =  getSourceRoot("JDK");
		assertTrue("The JDK_SOURCE_ROOT environment variable must be set for performance testing.", jdkSourceRoot != null && !jdkSourceRoot.isEmpty());

        compileJavaParser(USE_LR_GRAMMAR);
		lexerName: string =  USE_LR_GRAMMAR ? "JavaLRLexer" : "JavaLexer";
		parserName: string =  USE_LR_GRAMMAR ? "JavaLRParser" : "JavaParser";
		listenerName: string =  USE_LR_GRAMMAR ? "JavaLRBaseListener" : "JavaBaseListener";
		entryPoint: string =  "compilationUnit";
        factory: ParserFactory =  getParserFactory(lexerName, parserName, listenerName, entryPoint);

        if (!TOP_PACKAGE.isEmpty()) {
            jdkSourceRoot = jdkSourceRoot + '/' + TOP_PACKAGE.replace('.', '/');
        }

        let directory: File =  new File(jdkSourceRoot);
        assertTrue(directory.isDirectory());

		let filesFilter: FilenameFilter =  FilenameFilters.extension(".java", false);
		let directoriesFilter: FilenameFilter =  FilenameFilters.ALL_FILES;
		sources: List<InputDescriptor> =  loadSources(directory, filesFilter, directoriesFilter, RECURSIVE);

		for (let i = 0; i < PASSES; i++) {
			if (COMPUTE_TRANSITION_STATS) {
				totalTransitionsPerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)];
				computedTransitionsPerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)];

				if (DETAILED_DFA_STATE_STATS) {
					decisionInvocationsPerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)][];
					fullContextFallbackPerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)][];
					nonSllPerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)][];
					totalTransitionsPerDecisionPerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)][];
					computedTransitionsPerDecisionPerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)][];
					fullContextTransitionsPerDecisionPerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)][];
				}
			}

			if (COMPUTE_TIMING_STATS) {
				timePerFile[i] = new long[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)];
				tokensPerFile[i] = new int[Math.min(sources.size(), MAX_FILES_PER_PARSE_ITERATION)];
			}
		}
		System.out.format("Located %d source files.%n", sources.size());
		System.out.print(getOptionsDescription(TOP_PACKAGE));

		let executorService: ExecutorService =  Executors.newFixedThreadPool(FILE_GRANULARITY ? 1 : NUMBER_OF_THREADS, new NumberedThreadFactory());
		let passResults: List<Future<?>> =  new ArrayList<Future<?>>();
		passResults.add(executorService.submit(new Runnable() {
			@Override
			run(): void {
				try {
					parse1(0, factory, sources, SHUFFLE_FILES_AT_START);
				} catch (InterruptedException ex) {
					Logger.getLogger(TestPerformance.class.getName()).log(Level.SEVERE, null, ex);
				}
			}
		}));
        for (let i = 0; i < PASSES - 1; i++) {
            currentPass: number =  i + 1;
			passResults.add(executorService.submit(new Runnable() {
				@Override
				run(): void {
					if (CLEAR_DFA) {
						let index: number =  FILE_GRANULARITY ? 0 : ((NumberedThread)Thread.currentThread()).getThreadNumber();
						if (sharedLexers.length > 0 && sharedLexers[index] != null) {
							let atn: ATN =  sharedLexers[index].getATN();
							atn.clearDFA();
						}

						if (sharedParsers.length > 0 && sharedParsers[index] != null) {
							let atn: ATN =  sharedParsers[index].getATN();
							atn.clearDFA();
						}

						if (FILE_GRANULARITY) {
							Arrays.fill(sharedLexers, null);
							Arrays.fill(sharedParsers, null);
						}
					}

					try {
						parse2(currentPass, factory, sources, SHUFFLE_FILES_AFTER_ITERATIONS);
					} catch (InterruptedException ex) {
						Logger.getLogger(TestPerformance.class.getName()).log(Level.SEVERE, null, ex);
					}
				}
			}));
        }

		for (let passResult of passResults) {
			passResult.get();
		}

		executorService.shutdown();
		executorService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);

		if (COMPUTE_TRANSITION_STATS && SHOW_TRANSITION_STATS_PER_FILE) {
			computeTransitionStatistics();
		}

		if (COMPUTE_TIMING_STATS) {
			computeTimingStatistics();
		}

		sources.clear();
		if (PAUSE_FOR_HEAP_DUMP) {
			System.gc();
			console.log("Pausing before application exit.");
			try {
				Thread.sleep(4000);
			} catch (InterruptedException ex) {
				Logger.getLogger(TestPerformance.class.getName()).log(Level.SEVERE, null, ex);
			}
		}
    }

	/**
	 * Compute and print ATN/DFA transition statistics.
	 */
	private computeTransitionStatistics(): void {
		if (TRANSITION_RUNNING_AVERAGE) {
			for (let i = 0; i < PASSES; i++) {
				let data: number[] =  computedTransitionsPerFile[i];
				for (let j = 0; j < data.length - 1; j++) {
					data[j + 1] += data[j];
				}

				data = totalTransitionsPerFile[i];
				for (let j = 0; j < data.length - 1; j++) {
					data[j + 1] += data[j];
				}
			}
		}

		let sumNum: number[] =  new long[totalTransitionsPerFile[0].length];
		let sumDen: number[] =  new long[totalTransitionsPerFile[0].length];
		let sumNormalized: number[] =  new double[totalTransitionsPerFile[0].length];
		for (let i = 0; i < PASSES; i++) {
			let num: number[] =  computedTransitionsPerFile[i];
			let den: number[] =  totalTransitionsPerFile[i];
			for (let j = 0; j < den.length; j++) {
				sumNum[j] += num[j];
				sumDen[j] += den[j];
				if (den[j] > 0) {
					sumNormalized[j] += (double)num[j] / (double)den[j];
				}
			}
		}

		let weightedAverage: number[] =  new double[totalTransitionsPerFile[0].length];
		let average: number[] =  new double[totalTransitionsPerFile[0].length];
		for (let i = 0; i < average.length; i++) {
			if (sumDen[i] > 0) {
				weightedAverage[i] = (double)sumNum[i] / (double)sumDen[i];
			}
			else {
				weightedAverage[i] = 0;
			}

			average[i] = sumNormalized[i] / PASSES;
		}

		let low95: number[] =  new double[totalTransitionsPerFile[0].length];
		let high95: number[] =  new double[totalTransitionsPerFile[0].length];
		let low67: number[] =  new double[totalTransitionsPerFile[0].length];
		let high67: number[] =  new double[totalTransitionsPerFile[0].length];
		let stddev: number[] =  new double[totalTransitionsPerFile[0].length];
		for (let i = 0; i < stddev.length; i++) {
			let points: number[] =  new double[PASSES];
			for (let j = 0; j < PASSES; j++) {
				let totalTransitions: number =  totalTransitionsPerFile[j][i];
				if (totalTransitions > 0) {
					points[j] = ((double)computedTransitionsPerFile[j][i] / (double)totalTransitionsPerFile[j][i]);
				}
				else {
					points[j] = 0;
				}
			}

			Arrays.sort(points);

			averageValue: number =  TRANSITION_WEIGHTED_AVERAGE ? weightedAverage[i] : average[i];
			let value: number =  0;
			for (let j = 0; j < PASSES; j++) {
				let diff: number =  points[j] - averageValue;
				value += diff * diff;
			}

			let ignoreCount95: number =  (int)Math.round(PASSES * (1 - 0.95) / 2.0);
			let ignoreCount67: number =  (int)Math.round(PASSES * (1 - 0.667) / 2.0);
			low95[i] = points[ignoreCount95];
			high95[i] = points[points.length - 1 - ignoreCount95];
			low67[i] = points[ignoreCount67];
			high67[i] = points[points.length - 1 - ignoreCount67];
			stddev[i] = Math.sqrt(value / PASSES);
		}

		System.out.format("File\tAverage\tStd. Dev.\t95%% Low\t95%% High\t66.7%% Low\t66.7%% High%n");
		for (let i = 0; i < stddev.length; i++) {
			averageValue: number =  TRANSITION_WEIGHTED_AVERAGE ? weightedAverage[i] : average[i];
			System.out.format("%d\t%e\t%e\t%e\t%e\t%e\t%e%n", i + 1, averageValue, stddev[i], averageValue - low95[i], high95[i] - averageValue, averageValue - low67[i], high67[i] - averageValue);
		}
	}

	/**
	 * Compute and print timing statistics.
	 */
	private computeTimingStatistics(): void {
		if (TIMING_CUMULATIVE) {
			for (let i = 0; i < PASSES; i++) {
				let data: number[] =  timePerFile[i];
				for (let j = 0; j < data.length - 1; j++) {
					data[j + 1] += data[j];
				}

				let data2: number[] =  tokensPerFile[i];
				for (let j = 0; j < data2.length - 1; j++) {
					data2[j + 1] += data2[j];
				}
			}
		}

		fileCount: number =  timePerFile[0].length;
		let sum: number[] =  new double[fileCount];
		for (let i = 0; i < PASSES; i++) {
			let data: number[] =  timePerFile[i];
			let tokenData: number[] =  tokensPerFile[i];
			for (let j = 0; j < data.length; j++) {
				sum[j] += (double)data[j] / (double)tokenData[j];
			}
		}

		let average: number[] =  new double[fileCount];
		for (let i = 0; i < average.length; i++) {
			average[i] = sum[i] / PASSES;
		}

		let low95: number[] =  new double[fileCount];
		let high95: number[] =  new double[fileCount];
		let low67: number[] =  new double[fileCount];
		let high67: number[] =  new double[fileCount];
		let stddev: number[] =  new double[fileCount];
		for (let i = 0; i < stddev.length; i++) {
			let points: number[] =  new double[PASSES];
			for (let j = 0; j < PASSES; j++) {
				points[j] = (double)timePerFile[j][i] / (double)tokensPerFile[j][i];
			}

			Arrays.sort(points);

			averageValue: number =  average[i];
			let value: number =  0;
			for (let j = 0; j < PASSES; j++) {
				let diff: number =  points[j] - averageValue;
				value += diff * diff;
			}

			let ignoreCount95: number =  (int)Math.round(PASSES * (1 - 0.95) / 2.0);
			let ignoreCount67: number =  (int)Math.round(PASSES * (1 - 0.667) / 2.0);
			low95[i] = points[ignoreCount95];
			high95[i] = points[points.length - 1 - ignoreCount95];
			low67[i] = points[ignoreCount67];
			high67[i] = points[points.length - 1 - ignoreCount67];
			stddev[i] = Math.sqrt(value / PASSES);
		}

		System.out.format("File\tAverage\tStd. Dev.\t95%% Low\t95%% High\t66.7%% Low\t66.7%% High%n");
		for (let i = 0; i < stddev.length; i++) {
			averageValue: number =  average[i];
			System.out.format("%d\t%e\t%e\t%e\t%e\t%e\t%e%n", i + 1, averageValue, stddev[i], averageValue - low95[i], high95[i] - averageValue, averageValue - low67[i], high67[i] - averageValue);
		}
	}

	private getSourceRoot(prefix: string): string {
		let sourceRoot: string =  System.getenv(prefix+"_SOURCE_ROOT");
		if (sourceRoot == null) {
			sourceRoot = System.getProperty(prefix+"_SOURCE_ROOT");
		}

		return sourceRoot;
	}

    @Override
    protected eraseTempDir(): void {
        if (DELETE_TEMP_FILES) {
            super.eraseTempDir();
        }
    }

    static getOptionsDescription(topPackage: string): string {
        let builder: StringBuilder =  new StringBuilder();
        builder.append("Input=");
        if (topPackage.isEmpty()) {
            builder.append("*");
        }
        else {
            builder.append(topPackage).append(".*");
        }

        builder.append(", Grammar=").append(USE_LR_GRAMMAR ? "LR" : "Standard");
        builder.append(", ForceAtn=").append(FORCE_ATN);
		builder.append(", Lexer:").append(ENABLE_LEXER_DFA ? "DFA" : "ATN");
		builder.append(", Parser:").append(ENABLE_PARSER_DFA ? "DFA" : "ATN");

        builder.append(newline);

        builder.append("Op=Lex").append(RUN_PARSER ? "+Parse" : " only");
        builder.append(", Strategy=").append(BAIL_ON_ERROR ? BailErrorStrategy.class.getSimpleName() : DefaultErrorStrategy.class.getSimpleName());
        builder.append(", BuildParseTree=").append(BUILD_PARSE_TREES);
        builder.append(", WalkBlankListener=").append(BLANK_LISTENER);

        builder.append(newline);

        builder.append("Lexer=").append(REUSE_LEXER ? "setInputStream" : "newInstance");
        builder.append(", Parser=").append(REUSE_PARSER ? "setInputStream" : "newInstance");
        builder.append(", AfterPass=").append(CLEAR_DFA ? "newInstance" : "setInputStream");

        builder.append('\n');

		builder.append("UniqueClosure=").append(OPTIMIZE_UNIQUE_CLOSURE ? "optimize" : "complete");

        builder.append(newline);

        return builder.toString();
    }

    /**
     *  This method is separate from {@link #parse2} so the first pass can be distinguished when analyzing
     *  profiler results.
     */
    protected parse1(currentPass: number, factory: ParserFactory, sources: Collection<InputDescriptor>, shuffleSources: boolean): void {
		if (FILE_GRANULARITY) {
			System.gc();
		}

        parseSources(currentPass, factory, sources, shuffleSources);
    }

    /**
     *  This method is separate from {@link #parse1} so the first pass can be distinguished when analyzing
     *  profiler results.
     */
    protected parse2(currentPass: number, factory: ParserFactory, sources: Collection<InputDescriptor>, shuffleSources: boolean): void {
		if (FILE_GRANULARITY) {
			System.gc();
		}

        parseSources(currentPass, factory, sources, shuffleSources);
    }

    protected loadSources(directory: File, filesFilter: FilenameFilter, directoriesFilter: FilenameFilter, recursive: boolean): List<InputDescriptor> {
        let result: List<InputDescriptor> =  new ArrayList<InputDescriptor>();
        loadSources(directory, filesFilter, directoriesFilter, recursive, result);
        return result;
    }

    protected loadSources(directory: File, filesFilter: FilenameFilter, directoriesFilter: FilenameFilter, recursive: boolean, result: Collection<InputDescriptor>): void {
        assert(directory.isDirectory());

        let sources: File[] =  directory.listFiles(filesFilter);
        for (let file of sources) {
			if (!file.isFile()) {
				continue;
			}

			result.add(new InputDescriptor(file.getAbsolutePath()));
        }

        if (recursive) {
            let children: File[] =  directory.listFiles(directoriesFilter);
            for (let child of children) {
                if (child.isDirectory()) {
                    loadSources(child, filesFilter, directoriesFilter, true, result);
                }
            }
        }
    }

    let configOutputSize: number =  0;

    @SuppressWarnings("unused")
	protected parseSources(final int currentPass,  final ParserFactory factory, sources: Collection<InputDescriptor>, shuffleSources: boolean): void {
		if (shuffleSources) {
			let sourcesList: List<InputDescriptor> =  new ArrayList<InputDescriptor>(sources);
			synchronized (RANDOM) {
				Collections.shuffle(sourcesList, RANDOM);
			}

			sources = sourcesList;
		}

		let startTime: number =  System.nanoTime();
        tokenCount.set(currentPass, 0);
        let inputSize: number =  0;
		let inputCount: number =  0;

		let results: Collection<Future<FileParseResult>> =  new ArrayList<Future<FileParseResult>>();
		let executorService: ExecutorService; 
		if (FILE_GRANULARITY) {
			executorService = Executors.newFixedThreadPool(FILE_GRANULARITY ? NUMBER_OF_THREADS : 1, new NumberedThreadFactory());
		} else {
			executorService = Executors.newSingleThreadExecutor(new FixedThreadNumberFactory(((NumberedThread)Thread.currentThread()).getThreadNumber()));
		}

        for (let inputDescriptor of sources) {
			if (inputCount >= MAX_FILES_PER_PARSE_ITERATION) {
				break;
			}

			input: CharStream =  inputDescriptor.getInputStream();
            input.seek(0);
            inputSize += input.size();
			inputCount++;
			let futureChecksum: Future<FileParseResult> =  executorService.submit(new Callable<FileParseResult>() {
				@Override
				call(): FileParseResult {
					// this incurred a great deal of overhead and was causing significant variations in performance results.
					//System.out.format("Parsing file %s\n", input.getSourceName());
					try {
						return factory.parseFile(input, currentPass, ((NumberedThread)Thread.currentThread()).getThreadNumber());
					} catch (IllegalStateException ex) {
						ex.printStackTrace(System.err);
					} catch (Throwable t) {
						t.printStackTrace(System.err);
					}

					return null;
				}
			});

			results.add(futureChecksum);
        }

		let checksum: Checksum =  new CRC32();
		let currentIndex: number =  -1;
		for (let future of results) {
			currentIndex++;
			let fileChecksum: number =  0;
			try {
				let fileResult: FileParseResult =  future.get();
				if (COMPUTE_TRANSITION_STATS) {
					totalTransitionsPerFile[currentPass][currentIndex] = sum(fileResult.parserTotalTransitions);
					computedTransitionsPerFile[currentPass][currentIndex] = sum(fileResult.parserComputedTransitions);

					if (DETAILED_DFA_STATE_STATS) {
						decisionInvocationsPerFile[currentPass][currentIndex] = fileResult.decisionInvocations;
						fullContextFallbackPerFile[currentPass][currentIndex] = fileResult.fullContextFallback;
						nonSllPerFile[currentPass][currentIndex] = fileResult.nonSll;
						totalTransitionsPerDecisionPerFile[currentPass][currentIndex] = fileResult.parserTotalTransitions;
						computedTransitionsPerDecisionPerFile[currentPass][currentIndex] = fileResult.parserComputedTransitions;
						fullContextTransitionsPerDecisionPerFile[currentPass][currentIndex] = fileResult.parserFullContextTransitions;
					}
				}

				if (COMPUTE_TIMING_STATS) {
					timePerFile[currentPass][currentIndex] = fileResult.endTime - fileResult.startTime;
					tokensPerFile[currentPass][currentIndex] = fileResult.tokenCount;
				}

				fileChecksum = fileResult.checksum;
			} catch (ExecutionException ex) {
				Logger.getLogger(TestPerformance.class.getName()).log(Level.SEVERE, null, ex);
			}

			if (COMPUTE_CHECKSUM) {
				updateChecksum(checksum, fileChecksum);
			}
		}

		executorService.shutdown();
		executorService.awaitTermination(Long.MAX_VALUE, TimeUnit.NANOSECONDS);

        System.out.format("%d. Total parse time for %d files (%d KB, %d tokens%s): %.0fms%n",
						  currentPass + 1,
                          inputCount,
                          inputSize / 1024,
                          tokenCount.get(currentPass),
						  COMPUTE_CHECKSUM ? String.format(", checksum 0x%8X", checksum.getValue()) : "",
                          (double)(System.nanoTime() - startTime) / 1000000.0);

		if (sharedLexers.length > 0) {
			let index: number =  FILE_GRANULARITY ? 0 : ((NumberedThread)Thread.currentThread()).getThreadNumber();
			let lexer: Lexer =  sharedLexers[index];
			lexerInterpreter: LexerATNSimulator =  lexer.getInterpreter();
			modeToDFA: DFA[] =  lexerInterpreter.atn.modeToDFA;
			if (SHOW_DFA_STATE_STATS) {
				let states: number =  0;
				let configs: number =  0;
				let uniqueConfigs: Set<ATNConfig> =  new HashSet<ATNConfig>();

				for (let i = 0; i < modeToDFA.length; i++) {
					let dfa: DFA =  modeToDFA[i];
					if (dfa == null) {
						continue;
					}

					states += dfa.states.size();
					for (let state of dfa.states.values()) {
						configs += state.configs.size();
						uniqueConfigs.addAll(state.configs);
					}
				}

				System.out.format("There are %d lexer DFAState instances, %d configs (%d unique), %d prediction contexts.%n", states, configs, uniqueConfigs.size(), lexerInterpreter.atn.getContextCacheSize());

				if (DETAILED_DFA_STATE_STATS) {
					System.out.format("\tMode\tStates\tConfigs\tMode%n");
					for (let i = 0; i < modeToDFA.length; i++) {
						let dfa: DFA =  modeToDFA[i];
						if (dfa == null || dfa.states.isEmpty()) {
							continue;
						}

						let modeConfigs: number =  0;
						for (let state of dfa.states.values()) {
							modeConfigs += state.configs.size();
						}

						let modeName: string =  lexer.getModeNames()[i];
						System.out.format("\t%d\t%d\t%d\t%s%n", dfa.decision, dfa.states.size(), modeConfigs, modeName);
					}
				}
			}
		}

		if (RUN_PARSER && sharedParsers.length > 0) {
			let index: number =  FILE_GRANULARITY ? 0 : ((NumberedThread)Thread.currentThread()).getThreadNumber();
			let parser: Parser =  sharedParsers[index];
            // make sure the individual DFAState objects actually have unique ATNConfig arrays
			interpreter: ParserATNSimulator =  parser.getInterpreter();
            decisionToDFA: DFA[] =  interpreter.atn.decisionToDFA;

            if (SHOW_DFA_STATE_STATS) {
                let states: number =  0;
				let configs: number =  0;
				let uniqueConfigs: Set<ATNConfig> =  new HashSet<ATNConfig>();

                for (let i = 0; i < decisionToDFA.length; i++) {
                    let dfa: DFA =  decisionToDFA[i];
                    if (dfa == null) {
                        continue;
                    }

                    states += dfa.states.size();
					for (let state of dfa.states.values()) {
						configs += state.configs.size();
						uniqueConfigs.addAll(state.configs);
					}
                }

                System.out.format("There are %d parser DFAState instances, %d configs (%d unique), %d prediction contexts.%n", states, configs, uniqueConfigs.size(), interpreter.atn.getContextCacheSize());

				if (DETAILED_DFA_STATE_STATS) {
					if (COMPUTE_TRANSITION_STATS) {
						System.out.format("\tDecision\tStates\tConfigs\tPredict (ALL)\tPredict (LL)\tNon-SLL\tTransitions\tTransitions (ATN)\tTransitions (LL)\tLA (SLL)\tLA (LL)\tRule%n");
					}
					else {
						System.out.format("\tDecision\tStates\tConfigs\tRule%n");
					}

					for (let i = 0; i < decisionToDFA.length; i++) {
						let dfa: DFA =  decisionToDFA[i];
						if (dfa == null || dfa.states.isEmpty()) {
							continue;
						}

						let decisionConfigs: number =  0;
						for (let state of dfa.states.values()) {
							decisionConfigs += state.configs.size();
						}

						let ruleName: string =  parser.getRuleNames()[parser.getATN().decisionToState.get(dfa.decision).ruleIndex];

						let calls: number =  0;
						let fullContextCalls: number =  0;
						let nonSllCalls: number =  0;
						let transitions: number =  0;
						let computedTransitions: number =  0;
						let fullContextTransitions: number =  0;
						let lookahead: number =  0;
						let fullContextLookahead: number =  0;
						let formatString: string; 
						if (COMPUTE_TRANSITION_STATS) {
							for (let data of decisionInvocationsPerFile[currentPass]) {
								calls += data[i];
							}

							for (let data of fullContextFallbackPerFile[currentPass]) {
								fullContextCalls += data[i];
							}

							for (let data of nonSllPerFile[currentPass]) {
								nonSllCalls += data[i];
							}

							for (let data of totalTransitionsPerDecisionPerFile[currentPass]) {
								transitions += data[i];
							}

							for (let data of computedTransitionsPerDecisionPerFile[currentPass]) {
								computedTransitions += data[i];
							}

							for (let data of fullContextTransitionsPerDecisionPerFile[currentPass]) {
								fullContextTransitions += data[i];
							}

							if (calls > 0) {
								lookahead = (double)(transitions - fullContextTransitions) / (double)calls;
							}

							if (fullContextCalls > 0) {
								fullContextLookahead = (double)fullContextTransitions / (double)fullContextCalls;
							}

							formatString = "\t%1$d\t%2$d\t%3$d\t%4$d\t%5$d\t%6$d\t%7$d\t%8$d\t%9$d\t%10$f\t%11$f\t%12$s%n";
						}
						else {
							calls = 0;
							formatString = "\t%1$d\t%2$d\t%3$d\t%12$s%n";
						}

						System.out.format(formatString, dfa.decision, dfa.states.size(), decisionConfigs, calls, fullContextCalls, nonSllCalls, transitions, computedTransitions, fullContextTransitions, lookahead, fullContextLookahead, ruleName);
					}
				}
            }

            let localDfaCount: number =  0;
            let globalDfaCount: number =  0;
            let localConfigCount: number =  0;
            let globalConfigCount: number =  0;
            let contextsInDFAState: number[] =  new int[0];

            for (let i = 0; i < decisionToDFA.length; i++) {
                let dfa: DFA =  decisionToDFA[i];
                if (dfa == null) {
                    continue;
                }

                if (SHOW_CONFIG_STATS) {
                    for (let state of dfa.states.keySet()) {
                        if (state.configs.size() >= contextsInDFAState.length) {
                            contextsInDFAState = Arrays.copyOf(contextsInDFAState, state.configs.size() + 1);
                        }

                        if (state.isAcceptState()) {
                            let hasGlobal: boolean =  false;
                            for (let config of state.configs) {
                                if (config.getReachesIntoOuterContext()) {
                                    globalConfigCount++;
                                    hasGlobal = true;
                                } else {
                                    localConfigCount++;
                                }
                            }

                            if (hasGlobal) {
                                globalDfaCount++;
                            } else {
                                localDfaCount++;
                            }
                        }

                        contextsInDFAState[state.configs.size()]++;
                    }
                }

                if (EXPORT_LARGEST_CONFIG_CONTEXTS) {
                    for (let state of dfa.states.keySet()) {
                        for (let config of state.configs) {
                            let configOutput: string =  config.toDotString();
                            if (configOutput.length() <= configOutputSize) {
                                continue;
                            }

                            configOutputSize = configOutput.length();
                            writeFile(tmpdir, "d" + dfa.decision + ".s" + state.stateNumber + ".a" + config.getAlt() + ".config.dot", configOutput);
                        }
                    }
                }
            }

            if (SHOW_CONFIG_STATS && currentPass == 0) {
                System.out.format("  DFA accept states: %d total, %d with only local context, %d with a global context%n", localDfaCount + globalDfaCount, localDfaCount, globalDfaCount);
                System.out.format("  Config stats: %d total, %d local, %d global%n", localConfigCount + globalConfigCount, localConfigCount, globalConfigCount);
                if (SHOW_DFA_STATE_STATS) {
                    for (let i = 0; i < contextsInDFAState.length; i++) {
                        if (contextsInDFAState[i] != 0) {
                            System.out.format("  %d configs = %d%n", i, contextsInDFAState[i]);
                        }
                    }
                }
            }
        }

		if (COMPUTE_TIMING_STATS) {
			System.out.format("File\tTokens\tTime%n");
			for (let i = 0; i< timePerFile[currentPass].length; i++) {
				System.out.format("%d\t%d\t%d%n", i + 1, tokensPerFile[currentPass][i], timePerFile[currentPass][i]);
			}
		}
    }

	private static sum(array: number[]): number {
		let result: number =  0;
		for (let i = 0; i < array.length; i++) {
			result += array[i];
		}

		return result;
	}

    protected compileJavaParser(leftRecursive: boolean): void {
        let grammarFileName: string =  leftRecursive ? "JavaLR.g4" : "Java.g4";
        let parserName: string =  leftRecursive ? "JavaLRParser" : "JavaParser";
        let lexerName: string =  leftRecursive ? "JavaLRLexer" : "JavaLexer";
        let body: string =  load(grammarFileName, null);
        let extraOptions: List<string> =  new ArrayList<String>();
		extraOptions.add("-Werror");
        if (FORCE_ATN) {
            extraOptions.add("-Xforce-atn");
        }
        if (EXPORT_ATN_GRAPHS) {
            extraOptions.add("-atn");
        }
		if (DEBUG_TEMPLATES) {
			extraOptions.add("-XdbgST");
			if (DEBUG_TEMPLATES_WAIT) {
				extraOptions.add("-XdbgSTWait");
			}
		}
		extraOptions.add("-visitor");
        let extraOptionsArray: string[] =  extraOptions.toArray(new String[extraOptions.size()]);
        let success: boolean =  rawGenerateAndBuildRecognizer(grammarFileName, body, parserName, lexerName, true, extraOptionsArray);
        assertTrue(success);
    }

	private static updateChecksum(checksum: Checksum, value: number): void {
		checksum.update((value) & 0xFF);
		checksum.update((value >>> 8) & 0xFF);
		checksum.update((value >>> 16) & 0xFF);
		checksum.update((value >>> 24) & 0xFF);
	}

	private static updateChecksum(checksum: Checksum, token: Token): void {
		if (token == null) {
			checksum.update(0);
			return;
		}

		updateChecksum(checksum, token.getStartIndex());
		updateChecksum(checksum, token.getStopIndex());
		updateChecksum(checksum, token.getLine());
		updateChecksum(checksum, token.getCharPositionInLine());
		updateChecksum(checksum, token.getType());
		updateChecksum(checksum, token.getChannel());
	}

    protected getParserFactory(lexerName: string, parserName: string, listenerName: string,  final String entryPoint): ParserFactory {
        try {
            let loader: ClassLoader =  new URLClassLoader(new URL[] { new File(tmpdir).toURI().toURL() }, ClassLoader.getSystemClassLoader());
            lexerClass: Class<? extends Lexer> =  loader.loadClass(lexerName).asSubclass(Lexer.class);
            parserClass: Class<? extends Parser> =  loader.loadClass(parserName).asSubclass(Parser.class);
            listenerClass: Class<? extends ParseTreeListener> =  (Class<? extends ParseTreeListener>)loader.loadClass(listenerName).asSubclass(ParseTreeListener.class);

            lexerCtor: Constructor<? extends Lexer> =  lexerClass.getConstructor(CharStream.class);
            parserCtor: Constructor<? extends Parser> =  parserClass.getConstructor(TokenStream.class);

            // construct initial instances of the lexer and parser to deserialize their ATNs
            let tokenSource: TokenSource =  lexerCtor.newInstance(new ANTLRInputStream(""));
            parserCtor.newInstance(new CommonTokenStream(tokenSource));

			if (!REUSE_LEXER_DFA) {
				let lexerSerializedATNField: Field =  lexerClass.getField("_serializedATN");
				let lexerSerializedATN: string =  (String)lexerSerializedATNField.get(null);
				for (let i = 0; i < NUMBER_OF_THREADS; i++) {
					sharedLexerATNs[i] = new ATNDeserializer().deserialize(lexerSerializedATN.toCharArray());
				}
			}

			if (RUN_PARSER && !REUSE_PARSER_DFA) {
				let parserSerializedATNField: Field =  parserClass.getField("_serializedATN");
				let parserSerializedATN: string =  (String)parserSerializedATNField.get(null);
				for (let i = 0; i < NUMBER_OF_THREADS; i++) {
					sharedParserATNs[i] = new ATNDeserializer().deserialize(parserSerializedATN.toCharArray());
				}
			}

            return new ParserFactory() {
                @SuppressWarnings("unused")
				@Override
                parseFile(input: CharStream, currentPass: number, thread: number): FileParseResult {
					checksum: Checksum =  new CRC32();

					startTime: number =  System.nanoTime();
					assert(thread >= 0 && thread < NUMBER_OF_THREADS);

                    try {
						let listener: ParseTreeListener =  sharedListeners[thread];
						if (listener == null) {
							listener = listenerClass.newInstance();
							sharedListeners[thread] = listener;
						}

						let lexer: Lexer =  sharedLexers[thread];
                        if (REUSE_LEXER && lexer != null) {
                            lexer.setInputStream(input);
                        } else {
							let previousLexer: Lexer =  lexer;
                            lexer = lexerCtor.newInstance(input);
							sharedLexers[thread] = lexer;
							let atn: ATN =  (FILE_GRANULARITY || previousLexer == null ? lexer : previousLexer).getATN();
							if (!REUSE_LEXER_DFA || (!FILE_GRANULARITY && previousLexer == null)) {
								atn = sharedLexerATNs[thread];
							}

							if (!ENABLE_LEXER_DFA) {
								lexer.setInterpreter(new NonCachingLexerATNSimulator(lexer, atn));
							} else if (!REUSE_LEXER_DFA || COMPUTE_TRANSITION_STATS) {
								lexer.setInterpreter(new StatisticsLexerATNSimulator(lexer, atn));
							}
                        }

						lexer.removeErrorListeners();
						lexer.addErrorListener(DescriptiveLexerErrorListener.INSTANCE);

						lexer.getInterpreter().optimize_tail_calls = OPTIMIZE_TAIL_CALLS;
						if (ENABLE_LEXER_DFA && !REUSE_LEXER_DFA) {
							lexer.getInterpreter().atn.clearDFA();
						}

                        let tokens: CommonTokenStream =  new CommonTokenStream(lexer);
                        tokens.fill();
                        tokenCount.addAndGet(currentPass, tokens.size());

						if (COMPUTE_CHECKSUM) {
							for (let token of tokens.getTokens()) {
								updateChecksum(checksum, token);
							}
						}

                        if (!RUN_PARSER) {
                            return new FileParseResult(input.getSourceName(), (int)checksum.getValue(), null, tokens.size(), startTime, lexer, null);
                        }

						parseStartTime: number =  System.nanoTime();
						let parser: Parser =  sharedParsers[thread];
                        if (REUSE_PARSER && parser != null) {
                            parser.setInputStream(tokens);
                        } else {
							let previousParser: Parser =  parser;

							if (USE_PARSER_INTERPRETER) {
								let referenceParser: Parser =  parserCtor.newInstance(tokens);
								parser = new ParserInterpreter(referenceParser.getGrammarFileName(), referenceParser.getVocabulary(), Arrays.asList(referenceParser.getRuleNames()), referenceParser.getATN(), tokens);
							}
							else {
								parser = parserCtor.newInstance(tokens);
							}

							let atn: ATN =  (FILE_GRANULARITY || previousParser == null ? parser : previousParser).getATN();
							if (!REUSE_PARSER_DFA || (!FILE_GRANULARITY && previousParser == null)) {
								atn = sharedLexerATNs[thread];
							}

							if (!ENABLE_PARSER_DFA) {
								parser.setInterpreter(new NonCachingParserATNSimulator(parser, atn));
							} else if (!REUSE_PARSER_DFA || COMPUTE_TRANSITION_STATS) {
								parser.setInterpreter(new StatisticsParserATNSimulator(parser, atn));
							}

                            sharedParsers[thread] = parser;
                        }

						parser.removeParseListeners();
						parser.removeErrorListeners();
						if (!TWO_STAGE_PARSING) {
							parser.addErrorListener(DescriptiveErrorListener.INSTANCE);
							parser.addErrorListener(new SummarizingDiagnosticErrorListener());
						}

						if (ENABLE_PARSER_DFA && !REUSE_PARSER_DFA) {
							parser.getInterpreter().atn.clearDFA();
						}

						parser.getInterpreter().setPredictionMode(TWO_STAGE_PARSING ? PredictionMode.SLL : PREDICTION_MODE);
						parser.getInterpreter().force_global_context = FORCE_GLOBAL_CONTEXT && !TWO_STAGE_PARSING;
						parser.getInterpreter().always_try_local_context = TRY_LOCAL_CONTEXT_FIRST || TWO_STAGE_PARSING;
						parser.getInterpreter().enable_global_context_dfa = ENABLE_PARSER_FULL_CONTEXT_DFA;
						parser.getInterpreter().optimize_ll1 = OPTIMIZE_LL1;
						parser.getInterpreter().optimize_unique_closure = OPTIMIZE_UNIQUE_CLOSURE;
						parser.getInterpreter().optimize_tail_calls = OPTIMIZE_TAIL_CALLS;
						parser.getInterpreter().tail_call_preserves_sll = TAIL_CALL_PRESERVES_SLL;
						parser.getInterpreter().treat_sllk1_conflict_as_ambiguity = TREAT_SLLK1_CONFLICT_AS_AMBIGUITY;
						parser.setBuildParseTree(BUILD_PARSE_TREES);
						if (!BUILD_PARSE_TREES && BLANK_LISTENER) {
							parser.addParseListener(listener);
						}
						if (BAIL_ON_ERROR || TWO_STAGE_PARSING) {
							parser.setErrorHandler(new BailErrorStrategy());
						}

                        let parseMethod: Method =  parserClass.getMethod(entryPoint);
                        let parseResult: any; 

						try {
							if (COMPUTE_CHECKSUM && !BUILD_PARSE_TREES) {
								parser.addParseListener(new ChecksumParseTreeListener(checksum));
							}

							if (USE_PARSER_INTERPRETER) {
								let parserInterpreter: ParserInterpreter =  (ParserInterpreter)parser;
								parseResult = parserInterpreter.parse(Collections.lastIndexOfSubList(Arrays.asList(parser.getRuleNames()), Collections.singletonList(entryPoint)));
							}
							else {
								parseResult = parseMethod.invoke(parser);
							}
						} catch (InvocationTargetException ex) {
							if (!TWO_STAGE_PARSING) {
								throw ex;
							}

							let sourceName: string =  tokens.getSourceName();
							sourceName = sourceName != null && !sourceName.isEmpty() ? sourceName+": " : "";
							if (REPORT_SECOND_STAGE_RETRY) {
								System.err.println(sourceName+"Forced to retry with full context.");
							}

							if (!(ex.getCause() instanceof ParseCancellationException)) {
								throw ex;
							}

							tokens.reset();
							if (REUSE_PARSER && sharedParsers[thread] != null) {
								parser.setInputStream(tokens);
							} else {
								if (USE_PARSER_INTERPRETER) {
									let referenceParser: Parser =  parserCtor.newInstance(tokens);
									parser = new ParserInterpreter(referenceParser.getGrammarFileName(), referenceParser.getVocabulary(), Arrays.asList(referenceParser.getRuleNames()), referenceParser.getATN(), tokens);
								}
								else {
									parser = parserCtor.newInstance(tokens);
								}

								sharedParsers[thread] = parser;
							}

							parser.removeParseListeners();
							parser.removeErrorListeners();
							parser.addErrorListener(DescriptiveErrorListener.INSTANCE);
							parser.addErrorListener(new SummarizingDiagnosticErrorListener());
							if (!ENABLE_PARSER_DFA) {
								parser.setInterpreter(new NonCachingParserATNSimulator(parser, parser.getATN()));
							} else if (!REUSE_PARSER_DFA) {
								parser.setInterpreter(new StatisticsParserATNSimulator(parser, sharedParserATNs[thread]));
							} else if (COMPUTE_TRANSITION_STATS) {
								parser.setInterpreter(new StatisticsParserATNSimulator(parser, parser.getATN()));
							}
							parser.getInterpreter().setPredictionMode(PREDICTION_MODE);
							parser.getInterpreter().force_global_context = FORCE_GLOBAL_CONTEXT;
							parser.getInterpreter().always_try_local_context = TRY_LOCAL_CONTEXT_FIRST;
							parser.getInterpreter().enable_global_context_dfa = ENABLE_PARSER_FULL_CONTEXT_DFA;
							parser.getInterpreter().optimize_ll1 = OPTIMIZE_LL1;
							parser.getInterpreter().optimize_unique_closure = OPTIMIZE_UNIQUE_CLOSURE;
							parser.getInterpreter().optimize_tail_calls = OPTIMIZE_TAIL_CALLS;
							parser.getInterpreter().tail_call_preserves_sll = TAIL_CALL_PRESERVES_SLL;
							parser.getInterpreter().treat_sllk1_conflict_as_ambiguity = TREAT_SLLK1_CONFLICT_AS_AMBIGUITY;
							parser.setBuildParseTree(BUILD_PARSE_TREES);
							if (COMPUTE_CHECKSUM && !BUILD_PARSE_TREES) {
								parser.addParseListener(new ChecksumParseTreeListener(checksum));
							}
							if (!BUILD_PARSE_TREES && BLANK_LISTENER) {
								parser.addParseListener(listener);
							}
							if (BAIL_ON_ERROR) {
								parser.setErrorHandler(new BailErrorStrategy());
							}

							parseResult = parseMethod.invoke(parser);
						}

						assertThat(parseResult, instanceOf(ParseTree.class));
						if (COMPUTE_CHECKSUM && BUILD_PARSE_TREES) {
							ParseTreeWalker.DEFAULT.walk(new ChecksumParseTreeListener(checksum), (ParseTree)parseResult);
						}
                        if (BUILD_PARSE_TREES && BLANK_LISTENER) {
                            ParseTreeWalker.DEFAULT.walk(listener, (ParserRuleContext)parseResult);
                        }

						return new FileParseResult(input.getSourceName(), (int)checksum.getValue(), (ParseTree)parseResult, tokens.size(), TIME_PARSE_ONLY ? parseStartTime : startTime, lexer, parser);
                    } catch (Exception e) {
						if (!REPORT_SYNTAX_ERRORS && e instanceof ParseCancellationException) {
							return new FileParseResult("unknown", (int)checksum.getValue(), null, 0, startTime, null, null);
						}

                        e.printStackTrace(System.out);
                        throw new IllegalStateException(e);
                    }
                }
            };
        } catch (Exception e) {
            e.printStackTrace(System.out);
            Assert.fail(e.getMessage());
            throw new IllegalStateException(e);
        }
    }

    protected interface ParserFactory {
        parseFile(input: CharStream, currentPass: number, thread: number): FileParseResult;
    }

	protected static class FileParseResult {
		sourceName: string; 
		checksum: number; 
		parseTree: ParseTree; 
		tokenCount: number; 
		startTime: number; 
		endTime: number; 

		lexerDFASize: number; 
		lexerTotalTransitions: number; 
		lexerComputedTransitions: number; 

		parserDFASize: number; 
		decisionInvocations: number[]; 
		fullContextFallback: number[]; 
		nonSll: number[]; 
		parserTotalTransitions: number[]; 
		parserComputedTransitions: number[]; 
		parserFullContextTransitions: number[]; 

		public FileParseResult(String sourceName, int checksum, @Nullable ParseTree parseTree, int tokenCount, long startTime, Lexer lexer, Parser parser) {
			this.sourceName = sourceName;
			this.checksum = checksum;
			this.parseTree = parseTree;
			this.tokenCount = tokenCount;
			this.startTime = startTime;
			this.endTime = System.nanoTime();

			if (lexer != null) {
				let interpreter: LexerATNSimulator =  lexer.getInterpreter();
				if (interpreter instanceof StatisticsLexerATNSimulator) {
					lexerTotalTransitions = ((StatisticsLexerATNSimulator)interpreter).totalTransitions;
					lexerComputedTransitions = ((StatisticsLexerATNSimulator)interpreter).computedTransitions;
				} else {
					lexerTotalTransitions = 0;
					lexerComputedTransitions = 0;
				}

				let dfaSize: number =  0;
				for (let dfa of interpreter.atn.decisionToDFA) {
					if (dfa != null) {
						dfaSize += dfa.states.size();
					}
				}

				lexerDFASize = dfaSize;
			} else {
				lexerDFASize = 0;
				lexerTotalTransitions = 0;
				lexerComputedTransitions = 0;
			}

			if (parser != null) {
				let interpreter: ParserATNSimulator =  parser.getInterpreter();
				if (interpreter instanceof StatisticsParserATNSimulator) {
					decisionInvocations = ((StatisticsParserATNSimulator)interpreter).decisionInvocations;
					fullContextFallback = ((StatisticsParserATNSimulator)interpreter).fullContextFallback;
					nonSll = ((StatisticsParserATNSimulator)interpreter).nonSll;
					parserTotalTransitions = ((StatisticsParserATNSimulator)interpreter).totalTransitions;
					parserComputedTransitions = ((StatisticsParserATNSimulator)interpreter).computedTransitions;
					parserFullContextTransitions = ((StatisticsParserATNSimulator)interpreter).fullContextTransitions;
				} else {
					decisionInvocations = new long[0];
					fullContextFallback = new long[0];
					nonSll = new long[0];
					parserTotalTransitions = new long[0];
					parserComputedTransitions = new long[0];
					parserFullContextTransitions = new long[0];
				}

				let dfaSize: number =  0;
				for (let dfa of interpreter.atn.decisionToDFA) {
					if (dfa != null) {
						dfaSize += dfa.states.size();
					}
				}

				parserDFASize = dfaSize;
			} else {
				parserDFASize = 0;
				decisionInvocations = new long[0];
				fullContextFallback = new long[0];
				nonSll = new long[0];
				parserTotalTransitions = new long[0];
				parserComputedTransitions = new long[0];
				parserFullContextTransitions = new long[0];
			}
		}
	}

	private static class StatisticsLexerATNSimulator extends LexerATNSimulator {

		totalTransitions: number; 
		computedTransitions: number; 

		public StatisticsLexerATNSimulator(ATN atn) {
			super(atn);
		}

		public StatisticsLexerATNSimulator(Lexer recog, ATN atn) {
			super(recog, atn);
		}

		@Override
		protected getExistingTargetState(s: DFAState, t: number): DFAState {
			totalTransitions++;
			return super.getExistingTargetState(s, t);
		}

		@Override
		protected computeTargetState(input: CharStream, s: DFAState, t: number): DFAState {
			computedTransitions++;
			return super.computeTargetState(input, s, t);
		}
	}

	private static class StatisticsParserATNSimulator extends ParserATNSimulator {

		decisionInvocations: number[]; 
		fullContextFallback: number[]; 
		nonSll: number[]; 
		totalTransitions: number[]; 
		computedTransitions: number[]; 
		fullContextTransitions: number[]; 

		private decision: number; 

		public StatisticsParserATNSimulator(ATN atn) {
			super(atn);
			decisionInvocations = new long[atn.decisionToState.size()];
			fullContextFallback = new long[atn.decisionToState.size()];
			nonSll = new long[atn.decisionToState.size()];
			totalTransitions = new long[atn.decisionToState.size()];
			computedTransitions = new long[atn.decisionToState.size()];
			fullContextTransitions = new long[atn.decisionToState.size()];
		}

		public StatisticsParserATNSimulator(Parser parser, ATN atn) {
			super(parser, atn);
			decisionInvocations = new long[atn.decisionToState.size()];
			fullContextFallback = new long[atn.decisionToState.size()];
			nonSll = new long[atn.decisionToState.size()];
			totalTransitions = new long[atn.decisionToState.size()];
			computedTransitions = new long[atn.decisionToState.size()];
			fullContextTransitions = new long[atn.decisionToState.size()];
		}

		@Override
		adaptivePredict(input: TokenStream, decision: number, outerContext: ParserRuleContext): number {
			try {
				this.decision = decision;
				decisionInvocations[decision]++;
				return super.adaptivePredict(input, decision, outerContext);
			}
			finally {
				this.decision = -1;
			}
		}

		@Override
		adaptivePredict(input: TokenStream, decision: number, outerContext: ParserRuleContext, useContext: boolean): number {
			if (useContext) {
				fullContextFallback[decision]++;
			}

			return super.adaptivePredict(input, decision, outerContext, useContext);
		}

		@Override
		protected getExistingTargetState(previousD: DFAState, t: number): DFAState {
			totalTransitions[decision]++;
			return super.getExistingTargetState(previousD, t);
		}

		@Override
		protected computeTargetState(dfa: DFA, s: DFAState, remainingGlobalContext: ParserRuleContext, t: number, useContext: boolean, contextCache: PredictionContextCache): Tuple2<DFAState, ParserRuleContext> {
			computedTransitions[decision]++;
			return super.computeTargetState(dfa, s, remainingGlobalContext, t, useContext, contextCache);
		}

		@Override
		protected computeReachSet(dfa: DFA, previous: SimulatorState, t: number, contextCache: PredictionContextCache): SimulatorState {
			if (previous.useContext) {
				totalTransitions[decision]++;
				computedTransitions[decision]++;
				fullContextTransitions[decision]++;
			}

			return super.computeReachSet(dfa, previous, t, contextCache);
		}
	}

	private static class DescriptiveErrorListener extends BaseErrorListener {
		static INSTANCE: DescriptiveErrorListener =  new DescriptiveErrorListener();

		@Override
		syntaxError<T extends Token>(recognizer: Recognizer<T,any>, offendingSymbol: T, line: number, charPositionInLine: number, msg: string, e: RecognitionException): void {
			if (!REPORT_SYNTAX_ERRORS) {
				return;
			}

			let sourceName: string =  recognizer.getInputStream().getSourceName();
			if (!sourceName.isEmpty()) {
				sourceName = String.format("%s:%d:%d: ", sourceName, line, charPositionInLine);
			}

			System.err.println(sourceName+"line "+line+":"+charPositionInLine+" "+msg);
		}

	}

	private static class DescriptiveLexerErrorListener implements ANTLRErrorListener<Integer> {
		static INSTANCE: DescriptiveLexerErrorListener =  new DescriptiveLexerErrorListener();

		@Override
		syntaxError<T extends Integer>(recognizer: Recognizer<T,any>, offendingSymbol: T, line: number, charPositionInLine: number, msg: string, e: RecognitionException): void {
			if (!REPORT_SYNTAX_ERRORS) {
				return;
			}

			let sourceName: string =  recognizer.getInputStream().getSourceName();
			if (!sourceName.isEmpty()) {
				sourceName = String.format("%s:%d:%d: ", sourceName, line, charPositionInLine);
			}

			System.err.println(sourceName+"line "+line+":"+charPositionInLine+" "+msg);
		}

	}

	private static class SummarizingDiagnosticErrorListener extends DiagnosticErrorListener {
		private BitSet _sllConflict;
		private ATNConfigSet _sllConfigs;

		@Override
		reportAmbiguity(recognizer: Parser, dfa: DFA, startIndex: number, stopIndex: number, exact: boolean, ambigAlts: BitSet, configs: ATNConfigSet): void {
			if (COMPUTE_TRANSITION_STATS && DETAILED_DFA_STATE_STATS) {
				let sllPredictions: BitSet =  getConflictingAlts(_sllConflict, _sllConfigs);
				let sllPrediction: number =  sllPredictions.nextSetBit(0);
				let llPredictions: BitSet =  getConflictingAlts(ambigAlts, configs);
				let llPrediction: number =  llPredictions.cardinality() == 0 ? ATN.INVALID_ALT_NUMBER : llPredictions.nextSetBit(0);
				if (sllPrediction != llPrediction) {
					((StatisticsParserATNSimulator)recognizer.getInterpreter()).nonSll[dfa.decision]++;
				}
			}

			if (!REPORT_AMBIGUITIES) {
				return;
			}

			// show the rule name along with the decision
			let format: string =  "reportAmbiguity d=%d (%s): ambigAlts=%s, input='%s'";
			let decision: number =  dfa.decision;
			let rule: string =  recognizer.getRuleNames()[dfa.atnStartState.ruleIndex];
			let input: string =  recognizer.getInputStream().getText(Interval.of(startIndex, stopIndex));
			recognizer.notifyErrorListeners(String.format(format, decision, rule, ambigAlts, input));
		}

		@Override
		reportAttemptingFullContext(recognizer: Parser, dfa: DFA, startIndex: number, stopIndex: number, conflictingAlts: BitSet, conflictState: SimulatorState): void {
			_sllConflict = conflictingAlts;
			_sllConfigs = conflictState.s0.configs;
			if (!REPORT_FULL_CONTEXT) {
				return;
			}

			// show the rule name and viable configs along with the base info
			let format: string =  "reportAttemptingFullContext d=%d (%s), input='%s', viable=%s";
			let decision: number =  dfa.decision;
			let rule: string =  recognizer.getRuleNames()[dfa.atnStartState.ruleIndex];
			let input: string =  recognizer.getInputStream().getText(Interval.of(startIndex, stopIndex));
			let representedAlts: BitSet =  getConflictingAlts(conflictingAlts, conflictState.s0.configs);
			recognizer.notifyErrorListeners(String.format(format, decision, rule, input, representedAlts));
		}

		@Override
		reportContextSensitivity(recognizer: Parser, dfa: DFA, startIndex: number, stopIndex: number, prediction: number, acceptState: SimulatorState): void {
			if (COMPUTE_TRANSITION_STATS && DETAILED_DFA_STATE_STATS) {
				let sllPredictions: BitSet =  getConflictingAlts(_sllConflict, _sllConfigs);
				let sllPrediction: number =  sllPredictions.nextSetBit(0);
				if (sllPrediction != prediction) {
					((StatisticsParserATNSimulator)recognizer.getInterpreter()).nonSll[dfa.decision]++;
				}
			}

			if (!REPORT_CONTEXT_SENSITIVITY) {
				return;
			}

			// show the rule name and viable configs along with the base info
			let format: string =  "reportContextSensitivity d=%d (%s), input='%s', viable={%d}";
			let decision: number =  dfa.decision;
			let rule: string =  recognizer.getRuleNames()[dfa.atnStartState.ruleIndex];
			let input: string =  recognizer.getInputStream().getText(Interval.of(startIndex, stopIndex));
			recognizer.notifyErrorListeners(String.format(format, decision, rule, input, prediction));
		}
	}

	protected static final class FilenameFilters {
		static ALL_FILES: FilenameFilter =  new FilenameFilter() {

			@Override
			accept(dir: File, name: string): boolean {
				return true;
			}

		};

		static extension(extension: string): FilenameFilter {
			return extension(extension, true);
		}

		static extension(extension: string, caseSensitive: boolean): FilenameFilter {
			return new FileExtensionFilenameFilter(extension, caseSensitive);
		}

		static name(filename: string): FilenameFilter {
			return name(filename, true);
		}

		static name(filename: string, caseSensitive: boolean): FilenameFilter {
			return new FileNameFilenameFilter(filename, caseSensitive);
		}

		static all(FilenameFilter... filters): FilenameFilter {
			return new AllFilenameFilter(filters);
		}

		static any(FilenameFilter... filters): FilenameFilter {
			return new AnyFilenameFilter(filters);
		}

		static none(FilenameFilter... filters): FilenameFilter {
			return not(any(filters));
		}

		static not(filter: FilenameFilter): FilenameFilter {
			return new NotFilenameFilter(filter);
		}

		private FilenameFilters() {
		}

		protected static class FileExtensionFilenameFilter implements FilenameFilter {

			private extension: string; 
			private caseSensitive: boolean; 

			public FileExtensionFilenameFilter(String extension, boolean caseSensitive) {
				if (!extension.startsWith(".")) {
					extension = '.' + extension;
				}

				this.extension = extension;
				this.caseSensitive = caseSensitive;
			}

			@Override
			accept(dir: File, name: string): boolean {
				if (caseSensitive) {
					return name.endsWith(extension);
				} else {
					return name.toLowerCase().endsWith(extension);
				}
			}
		}

		protected static class FileNameFilenameFilter implements FilenameFilter {

			private filename: string; 
			private caseSensitive: boolean; 

			public FileNameFilenameFilter(String filename, boolean caseSensitive) {
				this.filename = filename;
				this.caseSensitive = caseSensitive;
			}

			@Override
			accept(dir: File, name: string): boolean {
				if (caseSensitive) {
					return name.equals(filename);
				} else {
					return name.toLowerCase().equals(filename);
				}
			}
		}

		protected static class AllFilenameFilter implements FilenameFilter {

			private filters: FilenameFilter[]; 

			public AllFilenameFilter(FilenameFilter[] filters) {
				this.filters = filters;
			}

			@Override
			accept(dir: File, name: string): boolean {
				for (let filter of filters) {
					if (!filter.accept(dir, name)) {
						return false;
					}
				}

				return true;
			}
		}

		protected static class AnyFilenameFilter implements FilenameFilter {

			private filters: FilenameFilter[]; 

			public AnyFilenameFilter(FilenameFilter[] filters) {
				this.filters = filters;
			}

			@Override
			accept(dir: File, name: string): boolean {
				for (let filter of filters) {
					if (filter.accept(dir, name)) {
						return true;
					}
				}

				return false;
			}
		}

		protected static class NotFilenameFilter implements FilenameFilter {

			private filter: FilenameFilter; 

			public NotFilenameFilter(FilenameFilter filter) {
				this.filter = filter;
			}

			@Override
			accept(dir: File, name: string): boolean {
				return !filter.accept(dir, name);
			}
		}
	}

	protected static class NonCachingLexerATNSimulator extends StatisticsLexerATNSimulator {

		public NonCachingLexerATNSimulator(Lexer recog, ATN atn) {
			super(recog, atn);
		}

		@Override
		protected addDFAState(configs: ATNConfigSet): DFAState {
			return null;
		}

	}

	protected static class NonCachingParserATNSimulator extends StatisticsParserATNSimulator {
		private static emptyMap: EmptyEdgeMap<DFAState> =  new EmptyEdgeMap<DFAState>(-1, -1);

		public NonCachingParserATNSimulator(Parser parser, ATN atn) {
			super(parser, atn);
		}

		@NotNull
		@Override
		protected createDFAState(@NotNull dfa: DFA, @NotNull configs: ATNConfigSet): DFAState {
			return new DFAState(emptyMap, dfa.getEmptyContextEdgeMap(), configs);
		}

	}

	protected static class NumberedThread extends Thread {
		private threadNumber: number; 

		public NumberedThread(Runnable target, int threadNumber) {
			super(target);
			this.threadNumber = threadNumber;
		}

		getThreadNumber(): number {
			return threadNumber;
		}

	}

	protected static class NumberedThreadFactory implements ThreadFactory {
		private nextThread: Atomicnumber =  new AtomicInteger();

		@Override
		newThread(r: Runnable): Thread {
			let threadNumber: number =  nextThread.getAndIncrement();
			assert(threadNumber < NUMBER_OF_THREADS);
			return new NumberedThread(r, threadNumber);
		}

	}

	protected static class FixedThreadNumberFactory implements ThreadFactory {
		private threadNumber: number; 

		public FixedThreadNumberFactory(int threadNumber) {
			this.threadNumber = threadNumber;
		}

		@Override
		newThread(r: Runnable): Thread {
			assert(threadNumber < NUMBER_OF_THREADS);
			return new NumberedThread(r, threadNumber);
		}
	}

	protected static class ChecksumParseTreeListener implements ParseTreeListener {
		private static VISIT_TERMINAL: number =  1;
		private static VISIT_ERROR_NODE: number =  2;
		private static ENTER_RULE: number =  3;
		private static EXIT_RULE: number =  4;

		private checksum: Checksum; 

		public ChecksumParseTreeListener(Checksum checksum) {
			this.checksum = checksum;
		}

		@Override
		visitTerminal(node: TerminalNode): void {
			checksum.update(VISIT_TERMINAL);
			updateChecksum(checksum, node.getSymbol());
		}

		@Override
		visitErrorNode(node: ErrorNode): void {
			checksum.update(VISIT_ERROR_NODE);
			updateChecksum(checksum, node.getSymbol());
		}

		@Override
		enterEveryRule(ctx: ParserRuleContext): void {
			checksum.update(ENTER_RULE);
			updateChecksum(checksum, ctx.getRuleIndex());
			updateChecksum(checksum, ctx.getStart());
		}

		@Override
		exitEveryRule(ctx: ParserRuleContext): void {
			checksum.update(EXIT_RULE);
			updateChecksum(checksum, ctx.getRuleIndex());
			updateChecksum(checksum, ctx.getStop());
		}

	}

	protected static final class InputDescriptor {
		private source: string; 
		private inputStream: Reference<CloneableANTLRFileStream>; 

		public InputDescriptor(@NotNull String source) {
			this.source = source;
			if (PRELOAD_SOURCES) {
				getInputStream();
			}
		}

		@NotNull
		getInputStream(): CharStream {
			let stream: CloneableANTLRFileStream =  inputStream != null ? inputStream.get() : null;
			if (stream == null) {
				try {
					stream = new CloneableANTLRFileStream(source, ENCODING);
				} catch (IOException ex) {
					Logger.getLogger(TestPerformance.class.getName()).log(Level.SEVERE, null, ex);
					throw new RuntimeException(ex);
				}

				if (PRELOAD_SOURCES) {
					inputStream = new StrongReference<CloneableANTLRFileStream>(stream);
				} else {
					inputStream = new SoftReference<CloneableANTLRFileStream>(stream);
				}
			}

			return new JavaUnicodeInputStream(stream.createCopy());
		}
	}

	protected static class CloneableANTLRFileStream extends ANTLRFileStream {

		public CloneableANTLRFileStream(String fileName, String encoding) {
			super(fileName, encoding);
		}

		createCopy(): ANTLRInputStream {
			let stream: ANTLRInputStream =  new ANTLRInputStream(this.data, this.n);
			stream.name = this.getSourceName();
			return stream;
		}
	}

	public static class StrongReference<T> extends WeakReference<T> {
		referent: T; 

		public StrongReference(T referent) {
			super(referent);
			this.referent = referent;
		}

		@Override
		get(): T {
			return referent;
		}
	}

	@Test(timeout = 20000)
	testExponentialInclude(): void {
		let grammarFormat: string = 
			"parser grammar Level_%d_%d;\n" +
			"\n" +
			"%s import Level_%d_1, Level_%d_2;\n" +
			"\n" +
			"rule_%d_%d : EOF;\n";

		console.log("dir "+tmpdir);
		mkdir(tmpdir);

		let startTime: number =  System.nanoTime();

		let levels: number =  20;
		for (let level = 0; level < levels; level++) {
			let leafPrefix: string =  level == levels - 1 ? "//" : "";
			let grammar1: string =  String.format(grammarFormat, level, 1, leafPrefix, level + 1, level + 1, level, 1);
			writeFile(tmpdir, "Level_" + level + "_1.g4", grammar1);
			if (level > 0) {
				let grammar2: string =  String.format(grammarFormat, level, 2, leafPrefix, level + 1, level + 1, level, 1);
				writeFile(tmpdir, "Level_" + level + "_2.g4", grammar2);
			}
		}

		let equeue: ErrorQueue =  antlr("Level_0_1.g4", false);
		Assert.assertTrue(equeue.errors.isEmpty());

		let endTime: number =  System.nanoTime();
		System.out.format("%s milliseconds.%n", (endTime - startTime) / 1000000.0);
	}
}
